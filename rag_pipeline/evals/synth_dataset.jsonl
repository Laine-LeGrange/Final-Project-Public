// Synthetic dataset created from GraphRAG paper for evaluation of RAG pipeline
// Each entry has: question, ground_truth answer, topic_id, user_id, and optional

{"question":"What problem does GraphRAG aim to solve compared to conventional vector RAG?","ground_truth":"GraphRAG targets global sensemaking questions that require corpus-wide understanding—such as themes and trends—where standard vector RAG, which retrieves only a few local passages, often fails. It builds a graph index and uses community summaries to answer such global queries.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"Summarize the high-level GraphRAG pipeline.","ground_truth":"(1) Chunk documents; (2) use an LLM to extract entities, relationships, and claims to build a knowledge graph; (3) run hierarchical community detection (e.g., Leiden); (4) generate bottom-up community summaries; (5) at query time, map over relevant communities and reduce partial answers into a single global answer.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"How does GraphRAG build its knowledge graph from text?","ground_truth":"An LLM extracts entities and relationships (and optionally claims) from text chunks. Extractions are aggregated into nodes and edges with descriptions and weights, deduplicated, and summarized to form the final graph index.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"What role does community detection play in GraphRAG?","ground_truth":"Community detection partitions the knowledge graph into nested groups of related entities, enabling hierarchical summarization and scalable, map-reduce style question answering across communities.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"Describe how GraphRAG generates community summaries.","ground_truth":"Elements (nodes, edges, claims) are prioritized by prominence; as many as fit are included in the prompt. Higher-level summaries replace details with shorter child summaries, producing bottom-up hierarchical reports that fit the context window.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"How is a final answer produced from community summaries?","ground_truth":"At query time, relevant community summaries are shuffled and chunked; a map step produces scored partial answers, zero-scored answers are dropped, and a reduce step concatenates the most helpful partials within the token budget to form a single global answer.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"Which evaluation criteria are used to compare GraphRAG to vector RAG for global sensemaking?","ground_truth":"The primary criteria are Comprehensiveness, Diversity, and Empowerment; Directness is included as a control criterion.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"Why include Directness as a control criterion?","ground_truth":"Directness favors concise, to-the-point answers and often trades off with comprehensiveness and diversity. Including it helps ensure improvements on global metrics aren’t merely verbosity.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"What datasets were used in the experiments and roughly how large were they?","ground_truth":"Two ~1M-token corpora: podcast transcripts and news articles, chunked to roughly 600 tokens per piece—representative of real-world, million-token-scale datasets.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"How did GraphRAG compare to vector RAG on global questions?","ground_truth":"GraphRAG variants substantially outperformed vector RAG on Comprehensiveness and Diversity across both datasets, while vector RAG tended to be more Direct.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"What is the token-efficiency advantage of root-level summaries (C0)?","ground_truth":"Root-level community summaries require dramatically fewer context tokens—on the order of 9×–43× less than shuffling full source texts—while still outperforming vector RAG on global metrics.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"How were claim-based metrics used to validate LLM-as-judge results?","ground_truth":"Unique factual claims were extracted from answers; average claims measured Comprehensiveness and clustered claims per answer measured Diversity. These claim-based results aligned with LLM judgments.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"Why does the paper use an 8k token window for generation?","ground_truth":"Empirically, an 8k window performed best for comprehensiveness and was comparable on other metrics, so 8k was fixed for community summaries and answering.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"What prompt technique helped improve entity recall at larger chunk sizes?","ground_truth":"A self-reflection step: the LLM first extracts entities, then is prompted to detect and add missed entities, preserving recall at larger chunk sizes without sacrificing quality.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"When should GraphRAG be preferred over classic vector-only RAG?","ground_truth":"Use GraphRAG for global/sensemaking tasks—discovering themes, trends, and cross-corpus connections. Use vector RAG for pinpoint, local factual lookups.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"What hybrid strategies are suggested for future work?","ground_truth":"Combine embedding search with just-in-time community report generation, and enable roll-up/drill-down across the community hierarchy for exploratory analysis.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"Name a limitation of the evaluation approach discussed in the paper.","ground_truth":"The evaluation focuses on two ~1M-token corpora; broader domains and additional measures (e.g., hallucination checks) would strengthen external validity.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
{"question":"In one sentence, what’s the core idea behind GraphRAG’s query answering?","ground_truth":"Summarize the corpus into hierarchical, community-level reports, answer locally within each community, then merge the best partials into a single global response.","topic_id":"5dc6c807-9692-4a40-ad32-32bc3f5c99c4","user_id":"00000000-0000-0000-0000-000000000000","meta":{"source":"GraphRAG paper (mock)"}}
